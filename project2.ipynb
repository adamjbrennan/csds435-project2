{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adamj\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adamj\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\adamj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages to kernel.\n",
    "\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.set_option('display.max_rows', 10)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSDS-435: Project #2\n",
    "# Adam Brennan and Charlie Song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_health_tweets_dataframe():\n",
    "    # read in the cnnhealth.txt file using utf-8 encoding\n",
    "    with open('cnnhealth.txt', 'r', encoding='utf-8') as cnn_health_tweets_dataset:\n",
    "        # initialize empty list to store tweets\n",
    "        tweets = []\n",
    "\n",
    "        # for each row in the dataset, skip the first two columns and append the tweet to the tweets list\n",
    "        for row in cnn_health_tweets_dataset:\n",
    "            columns = row.strip().split('|')\n",
    "            tweet = '|'.join(columns[2:])\n",
    "            tweets.append([tweet])\n",
    "\n",
    "    # convert list of tweets to pandas DataFrame\n",
    "    df = pd.DataFrame(tweets, columns=['tweet'])\n",
    "    \n",
    "    # return the resulting pandas DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words_matrix(tweets_list):\n",
    "    # initialize CountVectorizer opbject \n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # build bag of words matrix from tweets list\n",
    "    bag_of_words_matrix = vectorizer.fit_transform(tweets_list)\n",
    "    \n",
    "    return bag_of_words_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words_dataframe(tweets_list):\n",
    "    # initialize CountVectorizer opbject \n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # build bag of words matrix from tweets list\n",
    "    bag_of_words_matrix = vectorizer.fit_transform(tweets_list)\n",
    "\n",
    "    # convert bag of words matrix to a pandas DataFrame\n",
    "    bag_of_words_df = pd.DataFrame(bag_of_words_matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return bag_of_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(tweets_df, tweet_index):\n",
    "    # return the tweet at the specified index in string form\n",
    "    return str(tweets_df.iloc[tweet_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_total_tokens(tweets_list):\n",
    "    num_total_tokens = 0\n",
    "\n",
    "    for tweet in tweets_list:\n",
    "        num_total_tokens += len(tweet)\n",
    "\n",
    "    return num_total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_summary_dataframe(tweets_list):\n",
    "    num_tweets = len(tweets_list)\n",
    "    num_total_tokens = get_num_total_tokens(tweets_list)\n",
    "    num_unique_tokens = get_bag_of_words_dataframe(tweets_list).shape[1]\n",
    "    num_avg_tokens_per_tweet = num_total_tokens / num_tweets\n",
    "\n",
    "    data = [[num_tweets, num_total_tokens, num_unique_tokens, num_avg_tokens_per_tweet, 'CNN Health Tweets']]\n",
    "\n",
    "    columns = ['# Tweets', '# Total Tokens', '# Unique Tokens', 'Avg. Tokens Per Tweet', 'Description']\n",
    "\n",
    "    tweets_summary_dataframe = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    return tweets_summary_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_distance(bag_of_words_matrix):\n",
    "    bag_of_words_matrix_ed = euclidean_distances(bag_of_words_matrix)\n",
    "\n",
    "    np.savetxt(\"euclidean_distance.txt\", bag_of_words_matrix_ed, delimiter=',', fmt='%.2f')\n",
    "\n",
    "    return bag_of_words_matrix_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(bag_of_words_matrix):\n",
    "    bag_of_words_matrix_cs = cosine_similarity(bag_of_words_matrix)\n",
    "\n",
    "    np.savetxt(\"cosine_similarity.txt\", bag_of_words_matrix_cs, delimiter=',', fmt='%.2f')\n",
    "\n",
    "    return bag_of_words_matrix_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the \"cnnhealth.txt\" file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An abundance of online info can turn us into e-hypochondriacs. Or, worse, lead us to neglect getting the care we need http://cnn.it/1L1t1Fv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A plant-based diet that incorporates fish may be the key to preventing colorectal cancers: http://cnn.it/1xdpsjT http://pbs.twimg.com/media/CAARHEGWEAAJGz6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It doesn't take much to damage your hearing at a sports bar or nightclub. That's why a billion people are at risk. http://cnn.it/1BOphBk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @CNN: Forever young? Discover this island’s secrets to longevity on #TheWonderList w/ @BillWeirCNN  http://cnn.it/1GNdmqc https://t.co/…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @CNN: Is post-traumatic stress disorder in your genes? A simple blood test may one day help tell you http://cnn.it/1xls8w5 http://t.co/…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>RT @EverydayHealth: Want killer abs? @JillianMichaels shows you how get them: http://at.cnn.com/ubAkAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>Medicare at stake -- @sanjayguptaCNN talks about politicians' plans http://at.cnn.com/7nLSOm7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058</th>\n",
       "      <td>Ann Romney talks about her experience with MS http://at.cnn.com/khb6keC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>Make sure your first marathon isn't your last! http://at.cnn.com/liehiPl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4060</th>\n",
       "      <td>Robin Roberts' cancer diagnosis http://at.cnn.com/XNq3fhk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4061 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                tweet\n",
       "0                         An abundance of online info can turn us into e-hypochondriacs. Or, worse, lead us to neglect getting the care we need http://cnn.it/1L1t1Fv\n",
       "1     A plant-based diet that incorporates fish may be the key to preventing colorectal cancers: http://cnn.it/1xdpsjT http://pbs.twimg.com/media/CAARHEGWEAAJGz6.jpg\n",
       "2                            It doesn't take much to damage your hearing at a sports bar or nightclub. That's why a billion people are at risk. http://cnn.it/1BOphBk\n",
       "3                         RT @CNN: Forever young? Discover this island’s secrets to longevity on #TheWonderList w/ @BillWeirCNN  http://cnn.it/1GNdmqc https://t.co/…\n",
       "4                         RT @CNN: Is post-traumatic stress disorder in your genes? A simple blood test may one day help tell you http://cnn.it/1xls8w5 http://t.co/…\n",
       "...                                                                                                                                                               ...\n",
       "4056                                                           RT @EverydayHealth: Want killer abs? @JillianMichaels shows you how get them: http://at.cnn.com/ubAkAN\n",
       "4057                                                                    Medicare at stake -- @sanjayguptaCNN talks about politicians' plans http://at.cnn.com/7nLSOm7\n",
       "4058                                                                                          Ann Romney talks about her experience with MS http://at.cnn.com/khb6keC\n",
       "4059                                                                                         Make sure your first marathon isn't your last! http://at.cnn.com/liehiPl\n",
       "4060                                                                                                        Robin Roberts' cancer diagnosis http://at.cnn.com/XNq3fhk\n",
       "\n",
       "[4061 rows x 1 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get pandas DataFrame\n",
    "tweets_df = get_cnn_health_tweets_dataframe()\n",
    "\n",
    "# display DataFrame\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tweet as list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tweets\n",
    "tweets_list = tweets_df['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the \"bag of words\" pandas DataFrame using the previously generated tweets pandas DataFrame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>008relo</th>\n",
       "      <th>00fvvdw</th>\n",
       "      <th>00igyua</th>\n",
       "      <th>01</th>\n",
       "      <th>01tdt1o</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>...</th>\n",
       "      <th>zwwtu7</th>\n",
       "      <th>zxodiqo</th>\n",
       "      <th>zy53fec</th>\n",
       "      <th>zy7u11</th>\n",
       "      <th>zyam2xi</th>\n",
       "      <th>zyl</th>\n",
       "      <th>zyt5bpe</th>\n",
       "      <th>zzetat</th>\n",
       "      <th>zzgrrgf</th>\n",
       "      <th>zzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4060</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4061 rows × 11264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000  008relo  00fvvdw  00igyua  01  01tdt1o  02  03  04  05  ...  \\\n",
       "0       0        0        0        0   0        0   0   0   0   0  ...   \n",
       "1       0        0        0        0   0        0   0   0   0   0  ...   \n",
       "2       0        0        0        0   0        0   0   0   0   0  ...   \n",
       "3       0        0        0        0   0        0   0   0   0   0  ...   \n",
       "4       0        0        0        0   0        0   0   0   0   0  ...   \n",
       "...   ...      ...      ...      ...  ..      ...  ..  ..  ..  ..  ...   \n",
       "4056    0        0        0        0   0        0   0   0   0   0  ...   \n",
       "4057    0        0        0        0   0        0   0   0   0   0  ...   \n",
       "4058    0        0        0        0   0        0   0   0   0   0  ...   \n",
       "4059    0        0        0        0   0        0   0   0   0   0  ...   \n",
       "4060    0        0        0        0   0        0   0   0   0   0  ...   \n",
       "\n",
       "      zwwtu7  zxodiqo  zy53fec  zy7u11  zyam2xi  zyl  zyt5bpe  zzetat  \\\n",
       "0          0        0        0       0        0    0        0       0   \n",
       "1          0        0        0       0        0    0        0       0   \n",
       "2          0        0        0       0        0    0        0       0   \n",
       "3          0        0        0       0        0    0        0       0   \n",
       "4          0        0        0       0        0    0        0       0   \n",
       "...      ...      ...      ...     ...      ...  ...      ...     ...   \n",
       "4056       0        0        0       0        0    0        0       0   \n",
       "4057       0        0        0       0        0    0        0       0   \n",
       "4058       0        0        0       0        0    0        0       0   \n",
       "4059       0        0        0       0        0    0        0       0   \n",
       "4060       0        0        0       0        0    0        0       0   \n",
       "\n",
       "      zzgrrgf  zzzzz  \n",
       "0           0      0  \n",
       "1           0      0  \n",
       "2           0      0  \n",
       "3           0      0  \n",
       "4           0      0  \n",
       "...       ...    ...  \n",
       "4056        0      0  \n",
       "4057        0      0  \n",
       "4058        0      0  \n",
       "4059        0      0  \n",
       "4060        0      0  \n",
       "\n",
       "[4061 rows x 11264 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate \"bag of words\" pandas DataFrame\n",
    "bag_of_words_dataframe = get_bag_of_words_dataframe(tweets_list)\n",
    "\n",
    "# display \"bag of words\" pandas DataFrame\n",
    "bag_of_words_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the tweets summary DataFrame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Tweets</th>\n",
       "      <th># Total Tokens</th>\n",
       "      <th># Unique Tokens</th>\n",
       "      <th>Avg. Tokens Per Tweet</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4061</td>\n",
       "      <td>441036</td>\n",
       "      <td>11264</td>\n",
       "      <td>108.602807</td>\n",
       "      <td>CNN Health Tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Tweets  # Total Tokens  # Unique Tokens  Avg. Tokens Per Tweet  \\\n",
       "0      4061          441036            11264             108.602807   \n",
       "\n",
       "         Description  \n",
       "0  CNN Health Tweets  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate \"bag of words\" pandas DataFrame\n",
    "tweets_summary_dataframe = get_tweets_summary_dataframe(tweets_list)\n",
    "\n",
    "tweets_summary_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance measure #1: Eucliean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 6.4807407 , 6.55743852, ..., 6.        , 6.164414  ,\n",
       "        5.65685425],\n",
       "       [6.4807407 , 0.        , 6.40312424, ..., 5.65685425, 5.83095189,\n",
       "        5.29150262],\n",
       "       [6.55743852, 6.40312424, 0.        , ..., 5.74456265, 5.56776436,\n",
       "        5.38516481],\n",
       "       ...,\n",
       "       [6.        , 5.65685425, 5.74456265, ..., 0.        , 4.47213595,\n",
       "        3.74165739],\n",
       "       [6.164414  , 5.83095189, 5.56776436, ..., 4.47213595, 0.        ,\n",
       "        4.        ],\n",
       "       [5.65685425, 5.29150262, 5.38516481, ..., 3.74165739, 4.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_matrix = get_bag_of_words_matrix(tweets_list)\n",
    "\n",
    "bag_of_words_matrix_ed = get_euclidean_distance(bag_of_words_matrix)\n",
    "\n",
    "bag_of_words_matrix_ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance measure #2: Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.22222222, 0.21821789, ..., 0.1067521 , 0.0993808 ,\n",
       "        0.12830006],\n",
       "       [0.22222222, 1.        , 0.25458754, ..., 0.21350421, 0.1987616 ,\n",
       "        0.25660012],\n",
       "       [0.21821789, 0.25458754, 1.        , ..., 0.20965697, 0.29277002,\n",
       "        0.25197632],\n",
       "       ...,\n",
       "       [0.1067521 , 0.21350421, 0.20965697, ..., 1.        , 0.28644595,\n",
       "        0.36980013],\n",
       "       [0.0993808 , 0.1987616 , 0.29277002, ..., 0.28644595, 1.        ,\n",
       "        0.34426519],\n",
       "       [0.12830006, 0.25660012, 0.25197632, ..., 0.36980013, 0.34426519,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_matrix = get_bag_of_words_matrix(tweets_list)\n",
    "\n",
    "bag_of_words_matrix_cs = get_cosine_similarity(bag_of_words_matrix)\n",
    "\n",
    "bag_of_words_matrix_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using KMeans..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with euclidean distance as the distance measure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 16, 17, ...,  2, 17, 18], dtype=int32)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = get_bag_of_words_matrix(tweets_list)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 20, random_state = 0)\n",
    "kmeans.fit(bow)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Starting soon: Allergies getting you down? Join our #CNNAllergies Twitter chat at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>TODAY: Join our #CNNAllergies Twitter chat at 1:30pm EST with @AllergyReliefNY and ask all of your allergy questions http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>TODAY: Allergies getting you down? Join our #CNNAllergies Twitter chat at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Questions about #allergies? Ask us using hashtag #CNNAllergies. Chat is TOMORROW at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>Allergies getting you down? Join our #CNNAllergies chat TOMORROW at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Join our #CNNAllergies chat TOMORROW at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>Learn about #allergies tomorrow! Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>Allergies getting you down? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>Allergies getting you down? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>What do you want to know about #allergies? Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>What do you want to know about #allergies? Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>Allergies getting you down? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>@MelEdits Thanks for those #CNNAllergies Qs! We hope you'll join us for the chat on 4/23 at 1:30 p.m EST</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>What do you want to know about #allergies? Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>Join us &amp;amp; @namicommunicate @DrashmanCNN @KellyWallaceTV tmw for a chat on raising kids w/ mental illness: http://on.fb.me/1d5gkEa #CNNParents</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 tweets  \\\n",
       "999          Starting soon: Allergies getting you down? Join our #CNNAllergies Twitter chat at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1003       TODAY: Join our #CNNAllergies Twitter chat at 1:30pm EST with @AllergyReliefNY and ask all of your allergy questions http://on.fb.me/1k9W1v8   \n",
       "1005                 TODAY: Allergies getting you down? Join our #CNNAllergies Twitter chat at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1009       Questions about #allergies? Ask us using hashtag #CNNAllergies. Chat is TOMORROW at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1011                       Allergies getting you down? Join our #CNNAllergies chat TOMORROW at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1013                                                   Join our #CNNAllergies chat TOMORROW at 1:30pm EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1016                 Learn about #allergies tomorrow! Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8   \n",
       "1023          What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1026                     Allergies getting you down? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1027                     Allergies getting you down? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1029          What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1033       What do you want to know about #allergies? Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8   \n",
       "1035       What do you want to know about #allergies? Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8   \n",
       "1037          What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1041                     Allergies getting you down? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1044          What can you do to get allergy relief? Join our #CNNAllergies chat on 4/23 at 1:30 p.m. EST with @AllergyReliefNY http://on.fb.me/1k9W1v8   \n",
       "1064                                           @MelEdits Thanks for those #CNNAllergies Qs! We hope you'll join us for the chat on 4/23 at 1:30 p.m EST   \n",
       "1073       What do you want to know about #allergies? Send us your questions before our #CNNAllergies chat on 4/23 @ 1:30 pm ET http://on.fb.me/1k9W1v8   \n",
       "1696  Join us &amp; @namicommunicate @DrashmanCNN @KellyWallaceTV tmw for a chat on raising kids w/ mental illness: http://on.fb.me/1d5gkEa #CNNParents   \n",
       "\n",
       "      label  \n",
       "999       1  \n",
       "1003      1  \n",
       "1005      1  \n",
       "1009      1  \n",
       "1011      1  \n",
       "1013      1  \n",
       "1016      1  \n",
       "1023      1  \n",
       "1026      1  \n",
       "1027      1  \n",
       "1029      1  \n",
       "1033      1  \n",
       "1035      1  \n",
       "1037      1  \n",
       "1041      1  \n",
       "1044      1  \n",
       "1064      1  \n",
       "1073      1  \n",
       "1696      1  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'tweets': tweets_list,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with cosine similarity as the distance measure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  3,  7, ..., 20, 24, 20], dtype=int32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = get_bag_of_words_matrix(tweets_list)\n",
    "normalized_bow = normalize(bow, norm='l2', axis=1)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 30, random_state = 0)\n",
    "kmeans.fit(normalized_bow)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>What every new triathlete needs http://at.cnn.com/RVWV4hz Great advice from @chrissiesmiles #triwithme @CNNFitNation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>You're crazy! Overcoming others' negativity in training http://at.cnn.com/Btwtbtg @CNNFitNation @TriHardClevelan #triwithme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>What every new triathlete needs http://at.cnn.com/ZHPbrig Great advice from @chrissiesmiles #triwithme @CNNFitNation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>What I wish I'd known when training for a triathlon http://at.cnn.com/vl5cbhK @TriHardStacy #triwithme @CNNFitNation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>Happy Fitness Friday! Follow the @CNNFitNation team as they train for their first triathlon http://at.cnn.com/XQ55jCK #triwithme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>Sometimes, you just have to lose control http://at.cnn.com/HzNIiq7 #triwithme @TriHardTabitha @CNNFitNation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>Triathlete conquers first time in the ocean http://at.cnn.com/mF7mZM8 #triwithme @CNNFitNation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>Behind the scenes: Training for a triathlon http://at.cnn.com/GhLeMNn @Spotify @CNNFitNation #triwithme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>Following the @CNNFitNation team? Our fancy new interactive will help you track their progress! http://at.cnn.com/YPjdPgv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>5 rules for new triathletes. @CNNFitNation coach @aprils_awesome shares her inside knowledge http://at.cnn.com/yWScDn8 #triwithme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>Our @CNNFitNation team is in Florida this week. Check out these photos! http://at.cnn.com/FUgeVC0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>Check out this #storify with tweets from our @CNNFitNation team in Florida http://sfy.co/s5yh #triwithme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>Our @CNNFitNation team is in Florida this week. Check out these photos! http://at.cnn.com/dfx6iQa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>3 lessons from a triathlete-to-be http://at.cnn.com/MfnB86w #triwithme #getfit @CNNFitNation @TriHardDouglas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>Our @CNNFitNation team learned a few valuable lessons in eating healthy http://at.cnn.com/A7aHTcA #triwithme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>. @TriHardStacy overcomes his biggest excuse: 'I am not a runner' http://at.cnn.com/uDY72i0 #triwithme @CNNFitNation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>So proud! Our @CNNFitNation team kicked off their training by conquering a mountain http://at.cnn.com/YhSQRLE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>Race day tips for the @CNNFitNation team from Ironman champ @chrissiesmiles http://at.cnn.com/jGtRUb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>Bride's vows: 'In fitness and in health' (via @CNNFitNation ) http://at.cnn.com/mUMszX #triwithme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>Making your health a priority, even on the road http://at.cnn.com/YrJHFy #triwithme (via @CNNFitNation )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 tweets  \\\n",
       "2225               What every new triathlete needs http://at.cnn.com/RVWV4hz Great advice from @chrissiesmiles #triwithme @CNNFitNation   \n",
       "2228        You're crazy! Overcoming others' negativity in training http://at.cnn.com/Btwtbtg @CNNFitNation @TriHardClevelan #triwithme   \n",
       "2265               What every new triathlete needs http://at.cnn.com/ZHPbrig Great advice from @chrissiesmiles #triwithme @CNNFitNation   \n",
       "2421               What I wish I'd known when training for a triathlon http://at.cnn.com/vl5cbhK @TriHardStacy #triwithme @CNNFitNation   \n",
       "2427   Happy Fitness Friday! Follow the @CNNFitNation team as they train for their first triathlon http://at.cnn.com/XQ55jCK #triwithme   \n",
       "2544                        Sometimes, you just have to lose control http://at.cnn.com/HzNIiq7 #triwithme @TriHardTabitha @CNNFitNation   \n",
       "2603                                     Triathlete conquers first time in the ocean http://at.cnn.com/mF7mZM8 #triwithme @CNNFitNation   \n",
       "2653                            Behind the scenes: Training for a triathlon http://at.cnn.com/GhLeMNn @Spotify @CNNFitNation #triwithme   \n",
       "2654          Following the @CNNFitNation team? Our fancy new interactive will help you track their progress! http://at.cnn.com/YPjdPgv   \n",
       "2667  5 rules for new triathletes. @CNNFitNation coach @aprils_awesome shares her inside knowledge http://at.cnn.com/yWScDn8 #triwithme   \n",
       "2668                                  Our @CNNFitNation team is in Florida this week. Check out these photos! http://at.cnn.com/FUgeVC0   \n",
       "2679                           Check out this #storify with tweets from our @CNNFitNation team in Florida http://sfy.co/s5yh #triwithme   \n",
       "2686                                  Our @CNNFitNation team is in Florida this week. Check out these photos! http://at.cnn.com/dfx6iQa   \n",
       "2706                       3 lessons from a triathlete-to-be http://at.cnn.com/MfnB86w #triwithme #getfit @CNNFitNation @TriHardDouglas   \n",
       "2838                       Our @CNNFitNation team learned a few valuable lessons in eating healthy http://at.cnn.com/A7aHTcA #triwithme   \n",
       "3059               . @TriHardStacy overcomes his biggest excuse: 'I am not a runner' http://at.cnn.com/uDY72i0 #triwithme @CNNFitNation   \n",
       "3198                      So proud! Our @CNNFitNation team kicked off their training by conquering a mountain http://at.cnn.com/YhSQRLE   \n",
       "3933                               Race day tips for the @CNNFitNation team from Ironman champ @chrissiesmiles http://at.cnn.com/jGtRUb   \n",
       "3939                                  Bride's vows: 'In fitness and in health' (via @CNNFitNation ) http://at.cnn.com/mUMszX #triwithme   \n",
       "3942                           Making your health a priority, even on the road http://at.cnn.com/YrJHFy #triwithme (via @CNNFitNation )   \n",
       "\n",
       "      label  \n",
       "2225      0  \n",
       "2228      0  \n",
       "2265      0  \n",
       "2421      0  \n",
       "2427      0  \n",
       "2544      0  \n",
       "2603      0  \n",
       "2653      0  \n",
       "2654      0  \n",
       "2667      0  \n",
       "2668      0  \n",
       "2679      0  \n",
       "2686      0  \n",
       "2706      0  \n",
       "2838      0  \n",
       "3059      0  \n",
       "3198      0  \n",
       "3933      0  \n",
       "3939      0  \n",
       "3942      0  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'tweets': tweets_list,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "df[df['label'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Hierarhical Clustering..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with euclidean distance as the distance measure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with cosine similarity as the distance measure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using DBSCAN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with euclidean distance as the distance measure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with cosine similarity as the distance measure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using EM Clustering..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with euclidean distance as the distance measure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First cluster with cosine similarity as the distance measure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
